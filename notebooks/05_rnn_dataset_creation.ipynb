{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3b9711",
   "metadata": {},
   "source": [
    "# Create RNN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716cec",
   "metadata": {},
   "source": [
    "## Use YOLO model to predict Heldout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f1929",
   "metadata": {},
   "outputs": [],
   "source": "from ultralytics import YOLO\nfrom pathlib import Path\nimport json\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/splits/development.csv\")\n\nmodel = YOLO('../runs/detect/pill_imprint_final/weights/best.pt')\n\nimage_dir = Path('../data/pillbox_production_images_full_202008')\ndf['image_path'] = df['original_name'].apply(lambda x: image_dir / x if pd.notna(x) else None)\ndf['exists'] = df['image_path'].apply(lambda x: x.exists() if x else False)\n\nvalid_images = df[df['exists']]['image_path'].tolist()\n\npredictions = []\nbatch_size = 250\n\nprint(f\"Processing {len(valid_images)} images in batches of {batch_size}...\")\n\n\nfor i in range(0, len(valid_images), batch_size):\n    batch = valid_images[i:i+batch_size]\n    results = model.predict(\n        batch,\n        conf=0.15,\n        agnostic_nms=True,\n        verbose=False)\n    \n    for img_path, result in zip(batch, results):\n        detections = []\n        for box in result.boxes:\n            detections.append({\n                'class_id': int(box.cls),\n                'class_name': result.names[int(box.cls)].upper(),  # Convert to uppercase\n                'confidence': float(box.conf),\n                'bbox': box.xywhn.tolist()[0]\n            })\n        predictions.append({\n            'image': img_path.name,\n            'detections': detections\n        })\n    \n    print(f\"Processed {min(i+batch_size, len(valid_images))}/{len(valid_images)}\")\n\nwith open('../data/predictions/developement_yolo_pred.json', 'w') as f:\n    json.dump(predictions, f, indent=2)\n\nprint(f\"Saved predictions to data/predictions/developement_yolo_pred.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6eef4e",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport random\n\nwith open('../data/predictions/developement_yolo_pred.json') as f:\n    predictions = json.load(f)\n\nsamples = random.sample(predictions, 5)\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\n\nfor ax, pred in zip(axes, samples):\n    img_path = image_dir / pred['image']\n    img = Image.open(img_path)\n    ax.imshow(img)\n    \n    img_width, img_height = img.size\n    \n    for det in pred['detections']:\n        x_center, y_center, w, h = det['bbox']\n        x = (x_center - w/2) * img_width\n        y = (y_center - h/2) * img_height\n        \n        rect = patches.Rectangle((x, y), w * img_width, h * img_height,\n                                linewidth=2, edgecolor='red', facecolor='none')\n        ax.add_patch(rect)\n        \n        ax.text(x, y - 5, f\"{det['class_name']} ({det['confidence']:.2f})\",\n               color='red', fontsize=10, fontweight='bold',\n               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n    \n    ax.axis('off')\n    ax.set_title(pred['image'], fontsize=8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "0b6a1e1e",
   "metadata": {},
   "source": [
    "## Use ResNet model to predice on Heldout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9hmwj5xilb",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport json\n\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nclass MultiTaskResNet(nn.Module):\n    def __init__(self, num_shapes, num_colors, num_forms):\n        super().__init__()\n        self.backbone = models.resnet18(weights=None)\n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n        self.shape_head = nn.Linear(in_features, num_shapes)\n        self.color_head = nn.Linear(in_features, num_colors)\n        self.form_head = nn.Linear(in_features, num_forms)\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.shape_head(features), self.color_head(features), self.form_head(features)\n\ncheckpoint = torch.load('../resnet_model/pill_classifier_full.pth', weights_only=False)\nresnet_model = MultiTaskResNet(\n    checkpoint['num_shape_classes'], \n    checkpoint['num_color_classes'],\n    checkpoint['num_form_classes']\n).to(device)\nresnet_model.load_state_dict(checkpoint['model_state_dict'])\nresnet_model.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nwith open('../data/predictions/developement_yolo_pred.json') as f:\n    yolo_preds = json.load(f)\n\nresnet_preds = []\nfor i in tqdm(range(0, len(yolo_preds), 100)):\n    batch = yolo_preds[i:i+100]\n    imgs = torch.stack([transform(Image.open(image_dir / p['image']).convert('RGB')) for p in batch]).to(device)\n    \n    with torch.no_grad():\n        shape_out, color_out, form_out = resnet_model(imgs)\n    \n    for j, pred in enumerate(batch):\n        resnet_preds.append({\n            'image': pred['image'],\n            'shape': checkpoint['shape_encoder'].inverse_transform([shape_out[j].argmax().item()])[0],\n            'color': checkpoint['color_encoder'].inverse_transform([color_out[j].argmax().item()])[0],\n            'form': checkpoint['form_encoder'].inverse_transform([form_out[j].argmax().item()])[0]\n        })\n\nwith open('../data/predictions/developement_resnet_pred.json', 'w') as f:\n    json.dump(resnet_preds, f, indent=2)\n\nprint(f\"Saved {len(resnet_preds)} predictions\")"
  },
  {
   "cell_type": "markdown",
   "id": "af3c89af",
   "metadata": {},
   "source": [
    "# Preprocess Splimprint (ground truth labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60598143",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimage_dir = Path('../data/pillbox_production_images_full_202008')"
  },
  {
   "cell_type": "markdown",
   "id": "e9947c40",
   "metadata": {},
   "source": [
    "Remove _ from splimprint_clean saves as labels. This is because YOLO will not predict any _."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5jrq4y0ptsr",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load ground truth labels\ndf = pd.read_csv(\"../data/splits/development.csv\")\n\n# Remove underscores from splimprint_clean and save to 'labels' column\ndf['labels'] = df['splimprint_clean'].str.replace('_', '', regex=False)\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "id": "cde3b4b3",
   "metadata": {},
   "source": [
    "# Encode ResNet and YOLO predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d396d9",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Load YOLO predictions\nwith open('../data/predictions/developement_yolo_pred.json', 'r') as f:\n    yolo_predictions = json.load(f)\n\n# Load ResNet predictions\nwith open('../data/predictions/developement_resnet_pred.json', 'r') as f:\n    resnet_predictions = json.load(f)\n\n# Load ground truth labels from the cleaned df (from previous cells)\nlabel_dict = dict(zip(df['original_name'], df['labels']))\n\n# Filter: keep only images with ground truth labels AND detections\nyolo_filtered = []\nfor pred in yolo_predictions:\n    img_name = pred['image']\n    has_detections = len(pred.get('detections', [])) > 0\n    has_label = img_name in label_dict and pd.notna(label_dict[img_name])\n    \n    if has_detections and has_label:\n        yolo_filtered.append(pred)\n\nprint(f\"Filtered: {len(yolo_predictions)} -> {len(yolo_filtered)} images\")\nprint(f\"  (removed {len(yolo_predictions) - len(yolo_filtered)} without detections or labels)\")\n\n# Dynamically find all unique values\nall_chars = set()\nall_shapes = set()\nall_colors = set()\nall_forms = set()\n\nfor pred in yolo_filtered:\n    for det in pred.get('detections', []):\n        all_chars.add(det['class_name'].upper())\n\nfor pred in resnet_predictions:\n    all_shapes.add(pred['shape'])\n    all_colors.add(pred['color'])\n    all_forms.add(pred['form'])\n\n# Convert to sorted lists\nALL_CHARS = sorted(list(all_chars))\nALL_SHAPES = sorted(list(all_shapes))\nALL_COLORS = sorted(list(all_colors))\nALL_FORMS = sorted(list(all_forms))\n\nprint(f\"\\nFound {len(ALL_CHARS)} unique characters: {ALL_CHARS}\")\nprint(f\"Found {len(ALL_SHAPES)} unique shapes: {ALL_SHAPES}\")\nprint(f\"Found {len(ALL_COLORS)} unique colors: {ALL_COLORS}\")\nprint(f\"Found {len(ALL_FORMS)} unique forms: {ALL_FORMS}\")\n\n# Create one-hot encoders\nchar_encoder = OneHotEncoder(categories=[ALL_CHARS], sparse_output=False)\nchar_encoder.fit(np.array(ALL_CHARS).reshape(-1, 1))\n\nshape_encoder = OneHotEncoder(categories=[ALL_SHAPES], sparse_output=False)\nshape_encoder.fit(np.array(ALL_SHAPES).reshape(-1, 1))\n\ncolor_encoder = OneHotEncoder(categories=[ALL_COLORS], sparse_output=False)\ncolor_encoder.fit(np.array(ALL_COLORS).reshape(-1, 1))\n\nform_encoder = OneHotEncoder(categories=[ALL_FORMS], sparse_output=False)\nform_encoder.fit(np.array(ALL_FORMS).reshape(-1, 1))\n\n# Create lookup dict for ResNet predictions\nresnet_dict = {pred['image']: pred for pred in resnet_predictions}\n\ndef sort_boxes_left_to_right(boxes):\n    \"\"\"Sort bounding boxes left-to-right, top-to-bottom\"\"\"\n    if len(boxes) == 0:\n        return []\n    \n    # Extract centers\n    centers = [(box['bbox'][0], box['bbox'][1]) for box in boxes]\n    \n    # Sort by y first (rows), then x (columns within row)\n    # Use smaller multiplier for y to avoid over-prioritizing vertical position\n    sorted_indices = sorted(range(len(centers)), \n                           key=lambda i: (round(centers[i][1] * 10), centers[i][0]))\n    \n    return [boxes[i] for i in sorted_indices]\n\ndef process_single_image(yolo_pred, resnet_pred):\n    \"\"\"Process one image with both YOLO and ResNet predictions\"\"\"\n    detections = yolo_pred.get('detections', [])\n    sorted_boxes = sort_boxes_left_to_right(detections)\n    \n    # Encode ResNet features (context - same for all characters)\n    shape_ohe = shape_encoder.transform([[resnet_pred['shape']]])[0]\n    color_ohe = color_encoder.transform([[resnet_pred['color']]])[0]\n    form_ohe = form_encoder.transform([[resnet_pred['form']]])[0]\n    \n    # Process each character\n    sequences = []\n    for det in sorted_boxes:\n        # Character coordinates (normalized)\n        x_center, y_center, w, h = det['bbox']\n        \n        # Character one-hot encoding\n        char = det['class_name'].upper()\n        char_ohe = char_encoder.transform([[char]])[0]\n        \n        # Concatenate: [x, y, char_OHE, shape_OHE, color_OHE, form_OHE]\n        feature_vector = np.concatenate([\n            [x_center, y_center],\n            char_ohe,\n            shape_ohe,\n            color_ohe,\n            form_ohe\n        ])\n        sequences.append(feature_vector)\n    \n    return np.array(sequences)\n\n# Process all filtered images\nprocessed_data = []\nfor yolo_pred in yolo_filtered:\n    img_name = yolo_pred['image']\n    if img_name in resnet_dict:\n        features = process_single_image(yolo_pred, resnet_dict[img_name])\n        processed_data.append({\n            'image': img_name,\n            'features': features,\n            'num_chars': len(features),\n            'target_str': label_dict[img_name]\n        })\n\n# Convert to DataFrame\ndf_processed = pd.DataFrame(processed_data)\n\nfeature_dim = 2 + len(ALL_CHARS) + len(ALL_SHAPES) + len(ALL_COLORS) + len(ALL_FORMS)\nprint(f\"\\nProcessed {len(df_processed)} images\")\nprint(f\"Feature vector size: {feature_dim}\")\nprint(f\"  - Coordinates: 2\")\nprint(f\"  - Character OHE: {len(ALL_CHARS)}\")\nprint(f\"  - Shape OHE: {len(ALL_SHAPES)}\")\nprint(f\"  - Color OHE: {len(ALL_COLORS)}\")\nprint(f\"  - Form OHE: {len(ALL_FORMS)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "41089ce5",
   "metadata": {},
   "source": [
    "## Padding ResNet Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hkukmyzgah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      "  Shape: torch.Size([7154, 81, 66])\n",
      "  Max length: 81\n",
      "  Format: (num_samples, max_len, feature_dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Convert to list of tensors\n",
    "sequences = [torch.FloatTensor(feat) for feat in df_processed['features'].tolist()]\n",
    "\n",
    "# Pad sequences (batch_first=True for shape: [batch, seq_len, features])\n",
    "X_padded = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "print(f\"Padded sequences:\")\n",
    "print(f\"  Shape: {X_padded.shape}\")\n",
    "print(f\"  Max length: {X_padded.shape[1]}\")\n",
    "print(f\"  Format: (num_samples, max_len, feature_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2dcca",
   "metadata": {},
   "source": [
    "## Padding YOLO Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6p8g20a0p5v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (38 tokens):\n",
      "  ALL_CHARS: ['<EOS>', '<PAD>', '<SOS>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "  SOS_IDX: 2\n",
      "  EOS_IDX: 0\n",
      "  PAD_IDX: 1\n",
      "\n",
      "Target Encoding:\n",
      "  Total samples: 7154\n",
      "  X_padded shape: torch.Size([7154, 81, 66])\n",
      "  y_padded shape: torch.Size([7154, 33])\n",
      "  Max target length: 33\n",
      "\n",
      "Example:\n",
      "  Original: W\n",
      "  Encoded: [2, 34, 0]\n",
      "  Decoded: ['<SOS>', 'W', '<EOS>']\n",
      "  Padded: [2, 34, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens to ALL_CHARS\n",
    "special_tokens = ['<SOS>', '<EOS>', '<PAD>']\n",
    "if not any(t in ALL_CHARS for t in special_tokens):\n",
    "    # Insert at beginning\n",
    "    ALL_CHARS = special_tokens + ALL_CHARS\n",
    "    ALL_CHARS = sorted(ALL_CHARS, key=lambda x: (x[0] != '<', x))  # Keep special tokens first\n",
    "\n",
    "# Create character to index mapping\n",
    "char_to_idx = {char: idx for idx, char in enumerate(ALL_CHARS)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "SOS_IDX = char_to_idx['<SOS>']\n",
    "EOS_IDX = char_to_idx['<EOS>']\n",
    "PAD_IDX = char_to_idx['<PAD>']\n",
    "\n",
    "print(f\"Vocabulary ({len(ALL_CHARS)} tokens):\")\n",
    "print(f\"  ALL_CHARS: {ALL_CHARS}\")\n",
    "print(f\"  SOS_IDX: {SOS_IDX}\")\n",
    "print(f\"  EOS_IDX: {EOS_IDX}\")\n",
    "print(f\"  PAD_IDX: {PAD_IDX}\")\n",
    "\n",
    "# Convert target strings to sequences of indices with SOS and EOS\n",
    "def encode_target(text):\n",
    "    \"\"\"Encode target text as: <SOS> + characters + <EOS>\"\"\"\n",
    "    text = str(text).upper()\n",
    "    # Convert each character (use PAD for unknown characters)\n",
    "    char_indices = [char_to_idx.get(c, PAD_IDX) for c in text]\n",
    "    # Add SOS at start and EOS at end\n",
    "    return [SOS_IDX] + char_indices + [EOS_IDX]\n",
    "\n",
    "# Encode targets (already in df_processed)\n",
    "df_processed['target_encoded'] = df_processed['target_str'].apply(encode_target)\n",
    "\n",
    "# Pad target sequences with PAD token\n",
    "max_target_len = df_processed['target_encoded'].apply(len).max()\n",
    "y_padded = pad_sequence(\n",
    "    [torch.LongTensor(t) for t in df_processed['target_encoded'].tolist()],\n",
    "    batch_first=True,\n",
    "    padding_value=PAD_IDX\n",
    ")\n",
    "\n",
    "print(f\"\\nTarget Encoding:\")\n",
    "print(f\"  Total samples: {len(df_processed)}\")\n",
    "print(f\"  X_padded shape: {X_padded.shape}\")\n",
    "print(f\"  y_padded shape: {y_padded.shape}\")\n",
    "print(f\"  Max target length: {max_target_len}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Original: {df_processed.iloc[0]['target_str']}\")\n",
    "print(f\"  Encoded: {df_processed.iloc[0]['target_encoded']}\")\n",
    "print(f\"  Decoded: {[idx_to_char[i] for i in df_processed.iloc[0]['target_encoded']]}\")\n",
    "print(f\"  Padded: {y_padded[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34dbe7",
   "metadata": {},
   "source": [
    "# Split and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjd1rv2cdg",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\n\n# 80-10-10 split for RNN training\n# First split: 80% train, 20% temp\ntrain_idx, temp_idx = train_test_split(\n    range(len(df_processed)), \n    test_size=0.2, \n    random_state=42\n)\n\n# Second split: 20% temp -> 10% val, 10% test\nval_idx, test_idx = train_test_split(\n    temp_idx, \n    test_size=0.5,\n    random_state=42\n)\n\n# Create RNN dataset (train, val, test)\nX_train = X_padded[train_idx]\nX_val = X_padded[val_idx]\nX_test = X_padded[test_idx]\n\ny_train = y_padded[train_idx]\ny_val = y_padded[val_idx]\ny_test = y_padded[test_idx]\n\ndf_train = df_processed.iloc[train_idx].reset_index(drop=True)\ndf_val = df_processed.iloc[val_idx].reset_index(drop=True)\ndf_test = df_processed.iloc[test_idx].reset_index(drop=True)\n\nprint(f\"RNN Dataset Split:\")\nprint(f\"  Train: {len(X_train)} ({len(X_train)/len(df_processed)*100:.1f}%)\")\nprint(f\"  Val:   {len(X_val)} ({len(X_val)/len(df_processed)*100:.1f}%)\")\nprint(f\"  Test:  {len(X_test)} ({len(X_test)/len(df_processed)*100:.1f}%)\")\nprint(f\"  Total: {len(df_processed)}\")\n\n# Save RNN dataset\ntorch.save({\n    'X_train': X_train,\n    'X_val': X_val,\n    'X_test': X_test,\n    'y_train': y_train,\n    'y_val': y_val,\n    'y_test': y_test,\n    'df_train': df_train,\n    'df_val': df_val,\n    'df_test': df_test,\n    'char_to_idx': char_to_idx,\n    'idx_to_char': idx_to_char,\n    'char_encoder': char_encoder,\n    'shape_encoder': shape_encoder,\n    'color_encoder': color_encoder,\n    'form_encoder': form_encoder,\n    'ALL_CHARS': ALL_CHARS,\n    'ALL_SHAPES': ALL_SHAPES,\n    'ALL_COLORS': ALL_COLORS,\n    'ALL_FORMS': ALL_FORMS,\n    'max_len': X_padded.shape[1],\n    'max_target_len': max_target_len,\n    'feature_dim': X_padded.shape[2],\n    'SOS_IDX': SOS_IDX,\n    'EOS_IDX': EOS_IDX,\n    'PAD_IDX': PAD_IDX\n}, '../data/predictions/rnn_dataset.pt')\n\nprint(f\"\\n✓ Saved to data/predictions/rnn_dataset.pt\")\nprint(f\"  - Input shape: (batch, {X_padded.shape[1]}, {X_padded.shape[2]})\")\nprint(f\"  - Target shape: (batch, {max_target_len})\")\nprint(f\"  - Special tokens: SOS={SOS_IDX}, EOS={EOS_IDX}, PAD={PAD_IDX}\")"
  },
  {
   "cell_type": "markdown",
   "id": "5ri6x7nnvcc",
   "metadata": {},
   "source": [
    "# Example: How a Prediction is Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3n2y9afmff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE: Encoding Pipeline\n",
      "================================================================================\n",
      "\n",
      "Image: 00555086002.jpg\n",
      "Target Label: 'B860100'\n",
      "Number of Characters Detected: 7\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 1: YOLO Character Detections (sorted left-to-right)\n",
      "--------------------------------------------------------------------------------\n",
      "  Char 1: 'B' at position (0.522, 0.228), confidence: 0.990\n",
      "  Char 2: '8' at position (0.320, 0.709), confidence: 0.989\n",
      "  Char 3: '6' at position (0.387, 0.707), confidence: 0.981\n",
      "  Char 4: '0' at position (0.451, 0.709), confidence: 0.947\n",
      "  Char 5: '1' at position (0.577, 0.708), confidence: 0.966\n",
      "  Char 6: '0' at position (0.629, 0.709), confidence: 0.949\n",
      "  Char 7: '0' at position (0.696, 0.711), confidence: 0.933\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 2: ResNet Global Predictions (same for all characters)\n",
      "--------------------------------------------------------------------------------\n",
      "  Shape: OVAL\n",
      "  Color: WHITE\n",
      "  Form:  C42998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 3: Encoding FIRST Character (in detail)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Character: 'B'\n",
      "  Position: (0.522, 0.228)\n",
      "\n",
      "  Character One-Hot (38 dims): [          0           0           0           0           0           0           0           0           0           0           0           1           0           0           0           0           0           0           0           0           0           0           0           0           0           0\n",
      "           0           0           0           0           0           0           0           0           0]\n",
      "    → Index 11 corresponds to '8'\n",
      "\n",
      "  Shape One-Hot (8 dims): [          0           0           0           1           0           0           0           0]\n",
      "    → Index 3 corresponds to 'OVAL'\n",
      "\n",
      "  Color One-Hot (11 dims): [          0           0           0           0           0           0           0           0           0           1           0]\n",
      "    → Index 9 corresponds to 'WHITE'\n",
      "\n",
      "  Form One-Hot (10 dims): [          0           0           0           0           0           0           0           0           0           1]\n",
      "    → Index 9 corresponds to 'C42998'\n",
      "\n",
      "  Final Feature Vector (66 dims):\n",
      "    [x=0.522, y=0.228, char_ohe(35), shape_ohe(8), color_ohe(11), form_ohe(10)]\n",
      "    Shape: (66,)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 4: Full Sequence Encoding (all characters)\n",
      "--------------------------------------------------------------------------------\n",
      "  Sequence Length: 7\n",
      "  Feature Vector Dimension: 66\n",
      "  Sequence Shape: (7, 66)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 5: Target Label Encoding\n",
      "--------------------------------------------------------------------------------\n",
      "  Original: 'B860100'\n",
      "  Encoded:  [2, 14, 11, 9, 3, 4, 3, 3, 0]\n",
      "  Decoded:  ['<SOS>', 'B', '8', '6', '0', '1', '0', '0', '<EOS>']\n",
      "\n",
      "  Breakdown:\n",
      "    Position 0:  2 → '<SOS>'\n",
      "    Position 1: 14 → 'B'\n",
      "    Position 2: 11 → '8'\n",
      "    Position 3:  9 → '6'\n",
      "    Position 4:  3 → '0'\n",
      "    Position 5:  4 → '1'\n",
      "    Position 6:  3 → '0'\n",
      "    Position 7:  3 → '0'\n",
      "    Position 8:  0 → '<EOS>'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6: After Padding (for batching)\n",
      "--------------------------------------------------------------------------------\n",
      "  Input (X) Padded Shape: torch.Size([81, 66])\n",
      "    → (max_sequence_length=81, feature_dim=66)\n",
      "    → Padding added: 74 zeros\n",
      "\n",
      "  Target (y) Padded Shape: torch.Size([33])\n",
      "    → Original: [2, 14, 11, 9, 3, 4, 3, 3, 0]\n",
      "    → Padded:   [2, 14, 11, 9, 3, 4, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "    → Padding added: 24 PAD tokens (1)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Input:  7 characters → Padded to 81 timesteps\n",
      "        Each timestep has 66 features\n",
      "Target: 'B860100' → [2, 14, 11, 9, 3, 4, 3, 3, 0] → Padded to 33\n",
      "        Special tokens: <SOS>=2, <EOS>=0, <PAD>=1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clean and Simple: How a Prediction is Encoded\n",
    "================================================\n",
    "\n",
    "This shows the complete encoding pipeline for a single pill image:\n",
    "1. YOLO detects characters (e.g., 'A', '6', '7')\n",
    "2. ResNet predicts shape/color/form (e.g., OVAL, PINK, C42931)\n",
    "3. Each character gets encoded with position + one-hot encodings\n",
    "4. Target label is encoded with special tokens\n",
    "\"\"\"\n",
    "\n",
    "# Select a random example\n",
    "import random\n",
    "example_idx = random.randint(0, len(df_processed) - 1)\n",
    "example = df_processed.iloc[example_idx]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE: Encoding Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nImage: {example['image']}\")\n",
    "print(f\"Target Label: '{example['target_str']}'\")\n",
    "print(f\"Number of Characters Detected: {example['num_chars']}\")\n",
    "\n",
    "# Step 1: Show YOLO detections\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 1: YOLO Character Detections (sorted left-to-right)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "yolo_pred = next(p for p in yolo_filtered if p['image'] == example['image'])\n",
    "resnet_pred = resnet_dict[example['image']]\n",
    "\n",
    "sorted_boxes = sort_boxes_left_to_right(yolo_pred['detections'])\n",
    "for i, det in enumerate(sorted_boxes):\n",
    "    x, y, w, h = det['bbox']\n",
    "    print(f\"  Char {i+1}: '{det['class_name']}' at position ({x:.3f}, {y:.3f}), confidence: {det['confidence']:.3f}\")\n",
    "\n",
    "# Step 2: Show ResNet predictions\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 2: ResNet Global Predictions (same for all characters)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Shape: {resnet_pred['shape']}\")\n",
    "print(f\"  Color: {resnet_pred['color']}\")\n",
    "print(f\"  Form:  {resnet_pred['form']}\")\n",
    "\n",
    "# Step 3: Show encoding for FIRST character in detail\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 3: Encoding FIRST Character (in detail)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(sorted_boxes) > 0:\n",
    "    first_det = sorted_boxes[0]\n",
    "    x, y, w, h = first_det['bbox']\n",
    "    char = first_det['class_name'].upper()\n",
    "    \n",
    "    # Get encodings\n",
    "    char_ohe = char_encoder.transform([[char]])[0]\n",
    "    shape_ohe = shape_encoder.transform([[resnet_pred['shape']]])[0]\n",
    "    color_ohe = color_encoder.transform([[resnet_pred['color']]])[0]\n",
    "    form_ohe = form_encoder.transform([[resnet_pred['form']]])[0]\n",
    "    \n",
    "    print(f\"\\nCharacter: '{char}'\")\n",
    "    print(f\"  Position: ({x:.3f}, {y:.3f})\")\n",
    "    print(f\"\\n  Character One-Hot ({len(ALL_CHARS)} dims): {char_ohe}\")\n",
    "    print(f\"    → Index {np.argmax(char_ohe)} corresponds to '{ALL_CHARS[np.argmax(char_ohe)]}'\")\n",
    "    print(f\"\\n  Shape One-Hot ({len(ALL_SHAPES)} dims): {shape_ohe}\")\n",
    "    print(f\"    → Index {np.argmax(shape_ohe)} corresponds to '{ALL_SHAPES[np.argmax(shape_ohe)]}'\")\n",
    "    print(f\"\\n  Color One-Hot ({len(ALL_COLORS)} dims): {color_ohe}\")\n",
    "    print(f\"    → Index {np.argmax(color_ohe)} corresponds to '{ALL_COLORS[np.argmax(color_ohe)]}'\")\n",
    "    print(f\"\\n  Form One-Hot ({len(ALL_FORMS)} dims): {form_ohe}\")\n",
    "    print(f\"    → Index {np.argmax(form_ohe)} corresponds to '{ALL_FORMS[np.argmax(form_ohe)]}'\")\n",
    "    \n",
    "    # Concatenate\n",
    "    feature_vector = np.concatenate([[x, y], char_ohe, shape_ohe, color_ohe, form_ohe])\n",
    "    print(f\"\\n  Final Feature Vector ({len(feature_vector)} dims):\")\n",
    "    print(f\"    [x={x:.3f}, y={y:.3f}, char_ohe({len(char_ohe)}), shape_ohe({len(shape_ohe)}), color_ohe({len(color_ohe)}), form_ohe({len(form_ohe)})]\")\n",
    "    print(f\"    Shape: {feature_vector.shape}\")\n",
    "\n",
    "# Step 4: Show full sequence encoding\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 4: Full Sequence Encoding (all characters)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Sequence Length: {len(example['features'])}\")\n",
    "print(f\"  Feature Vector Dimension: {len(example['features'][0])}\")\n",
    "print(f\"  Sequence Shape: {example['features'].shape}\")\n",
    "\n",
    "# Step 5: Show target encoding\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 5: Target Label Encoding\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Original: '{example['target_str']}'\")\n",
    "print(f\"  Encoded:  {example['target_encoded']}\")\n",
    "print(f\"  Decoded:  {[idx_to_char[i] for i in example['target_encoded']]}\")\n",
    "print(f\"\\n  Breakdown:\")\n",
    "for i, idx in enumerate(example['target_encoded']):\n",
    "    print(f\"    Position {i}: {idx:2d} → '{idx_to_char[idx]}'\")\n",
    "\n",
    "# Step 6: Show padded version\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STEP 6: After Padding (for batching)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find this example in X_padded and y_padded\n",
    "original_idx = df_processed[df_processed['image'] == example['image']].index[0]\n",
    "X_sample = X_padded[original_idx]\n",
    "y_sample = y_padded[original_idx]\n",
    "\n",
    "print(f\"  Input (X) Padded Shape: {X_sample.shape}\")\n",
    "print(f\"    → (max_sequence_length={X_sample.shape[0]}, feature_dim={X_sample.shape[1]})\")\n",
    "print(f\"    → Padding added: {X_sample.shape[0] - len(example['features'])} zeros\")\n",
    "\n",
    "print(f\"\\n  Target (y) Padded Shape: {y_sample.shape}\")\n",
    "print(f\"    → Original: {example['target_encoded']}\")\n",
    "print(f\"    → Padded:   {y_sample.tolist()}\")\n",
    "print(f\"    → Padding added: {y_sample.shape[0] - len(example['target_encoded'])} PAD tokens ({PAD_IDX})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input:  {example['num_chars']} characters → Padded to {X_sample.shape[0]} timesteps\")\n",
    "print(f\"        Each timestep has {X_sample.shape[1]} features\")\n",
    "print(f\"Target: '{example['target_str']}' → {example['target_encoded']} → Padded to {y_sample.shape[0]}\")\n",
    "print(f\"        Special tokens: <SOS>=2, <EOS>=0, <PAD>=1\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}